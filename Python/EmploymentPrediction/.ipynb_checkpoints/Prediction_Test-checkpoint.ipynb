{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymysql \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metric_learn import LMNN\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import clone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8f7304-b4cd-41b7-9b3c-2fe59553ad0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化成功！连接成功！\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mydb=pymysql.connect(host='localhost',user='root',passwd='123456',port=3306,database='employmentprediction')\n",
    "    cursor=mydb.cursor()\n",
    "    print('初始化成功！连接成功！')\n",
    "except:\n",
    "    print('连接出错')\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5223b14-1659-41ad-8a47-42be7c77e1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaCost(object):\n",
    "\n",
    "    \"\"\"A procedure for making error-based classifiers cost-sensitive\n",
    "\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>> S = pd.DataFrame(load_iris().data)\n",
    "    >>> S['target'] = load_iris().target\n",
    "    >>> LR = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    >>> C = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n",
    "    >>> model = MetaCost(S, LR, C).fit('target', 3)\n",
    "    >>> model.predict_proba(load_iris().data[[2]])\n",
    "    >>> model.score(S[[0, 1, 2, 3]].values, S['target'])\n",
    "\n",
    "    .. note:: The form of the cost matrix C must be as follows:\n",
    "    +---------------+----------+----------+----------+\n",
    "    |  actual class |          |          |          |\n",
    "    +               |          |          |          |\n",
    "    |   +           | y(x)=j_1 | y(x)=j_2 | y(x)=j_3 |\n",
    "    |       +       |          |          |          |\n",
    "    |           +   |          |          |          |\n",
    "    |predicted class|          |          |          |\n",
    "    +---------------+----------+----------+----------+\n",
    "    |   h(x)=j_1    |    0     |    a     |     b    |\n",
    "    |   h(x)=j_2    |    c     |    0     |     d    |\n",
    "    |   h(x)=j_3    |    e     |    f     |     0    |\n",
    "    +---------------+----------+----------+----------+\n",
    "    | C = np.array([[0, a, b],[c, 0 , d],[e, f, 0]]) |\n",
    "    +------------------------------------------------+\n",
    "    \"\"\"\n",
    "    def __init__(self, S, L, C, m=50, n=1, p=True, q=True):\n",
    "        \"\"\"\n",
    "        :param S: The training set\n",
    "        :param L: A classification learning algorithm\n",
    "        :param C: A cost matrix\n",
    "        :param q: Is True iff all resamples are to be used  for each examples\n",
    "        :param m: The number of resamples to generate\n",
    "        :param n: The number of examples in each resample\n",
    "        :param p: Is True iff L produces class probabilities\n",
    "        \"\"\"\n",
    "        if not isinstance(S, pd.DataFrame):\n",
    "            raise ValueError('S must be a DataFrame object')\n",
    "        new_index = list(range(len(S)))\n",
    "        S.index = new_index\n",
    "        self.S = S\n",
    "        self.L = L\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        self.n = len(S) * n\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    def fit(self, flag, num_class):\n",
    "        \"\"\"\n",
    "        :param flag: The name of classification labels\n",
    "        :param num_class: The number of classes\n",
    "        :return: Classifier\n",
    "        \"\"\"\n",
    "        col = [col for col in self.S.columns if col != flag]\n",
    "        S_ = {}\n",
    "        M = []\n",
    "\n",
    "        for i in range(self.m):\n",
    "            # Let S_[i] be a resample of S with self.n examples\n",
    "            S_[i] = self.S.sample(n=self.n, replace=True)\n",
    "\n",
    "            X = S_[i][col].values\n",
    "            y = S_[i][flag].values\n",
    "\n",
    "            # Let M[i] = model produced by applying L to S_[i]\n",
    "            model = clone(self.L)\n",
    "            M.append(model.fit(X, y))\n",
    "\n",
    "        label = []\n",
    "        S_array = self.S[col].values\n",
    "        for i in range(len(self.S)):\n",
    "            if not self.q:\n",
    "                k_th = [k for k, v in S_.items() if i not in v.index]\n",
    "                M_ = list(np.array(M)[k_th])\n",
    "            else:\n",
    "                M_ = M\n",
    "\n",
    "            if self.p:\n",
    "                P_j = [model.predict_proba(S_array[[i]]) for model in M_]\n",
    "            else:\n",
    "                P_j = []\n",
    "                vector = [0] * num_class\n",
    "                for model in M_:\n",
    "                    vector[model.predict(S_array[[i]])] = 1\n",
    "                    P_j.append(vector)\n",
    "\n",
    "            # Calculate P(j|x)\n",
    "            P = np.array(np.mean(P_j, 0)).T\n",
    "\n",
    "            # Relabel\n",
    "            label.append(np.argmin(self.C.dot(P)))\n",
    "\n",
    "        # Model produced by applying L to S with relabeled y\n",
    "        X_train = self.S[col].values\n",
    "        y_train = np.array(label)\n",
    "        model_new = clone(self.L)\n",
    "        model_new.fit(X_train, y_train)\n",
    "\n",
    "        return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f57386b-8ea6-496d-b52c-492ac446cc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class imclassifier:\n",
    "    '在不平衡数据集中进行分类预测'\n",
    "    def __init__(self,data_original,feature_name,label_name,ir_threshold=4,dsc=True,dse=True,dfe=True,csl=True,el=True):\n",
    "        'data_original存储原始数据'\n",
    "        self.data_original=data_original\n",
    "        \n",
    "        'feature存储data_original中的特征列名'\n",
    "        self.feature_name=feature_name\n",
    "        \n",
    "        'label存储data_original中的标签列名'\n",
    "        self.label_name=label_name\n",
    "        \n",
    "        '''\n",
    "        标签占比阈值,\n",
    "        如果标签的占比大于阈值,\n",
    "        这个标签才会被选入筛选器\n",
    "        '''\n",
    "        # self.target_threshold=target_threshold\n",
    "        \n",
    "        'IR阈值'\n",
    "        self.ir_threshold=ir_threshold\n",
    "        \n",
    "        '变换同义的标签'\n",
    "        # self.data_original[label].replace(['其他暂不就业','不就业拟升学'],value='待就业',inplace=True)\n",
    "\n",
    "#         '对原始数据的标签进行统计'\n",
    "#         self.target_counts=self.data_original[self.label_name].value_counts(normalize=True)\n",
    "        \n",
    "#         'target_selected是index属性的,用此来筛选数据'\n",
    "#         self.target_selected=self.target_counts[self.target_counts>=self.target_threshold].index\n",
    "        \n",
    "#         'data_selected用于存储筛选后的数据'\n",
    "#         self.data_selected=self.data_original[self.data_original[self.label_name].isin(self.target_selected)]\n",
    "        \n",
    "        'feature用于存储所有的特征' \n",
    "        self.feature=self.data_original.loc[:,self.feature_name]\n",
    "        'target用于存储标签'\n",
    "        self.target=self.data_original.loc[:,self.label_name]\n",
    "\n",
    "        '为标签列编码'\n",
    "        self.target_o_encoder=OrdinalEncoder()\n",
    "        self.target_ordinal=pd.DataFrame(self.target_o_encoder.fit_transform(self.target.to_frame()),columns=[self.label_name])\n",
    "        \n",
    "        '编码器编码的标签名,target_name为数组属性'\n",
    "        self.target_name=self.target_o_encoder.categories_[0]\n",
    "\n",
    "        '经由编码器编码后的总标签数'\n",
    "        self.target_types=len(self.target_name)\n",
    "        \n",
    "        '''\n",
    "        DSC:Data Subset Construction\n",
    "        data_subset用于存储子数据集\n",
    "        data_subset_ir用于存储子数据集imbalance ratio\n",
    "        '''\n",
    "        self.data_subset=[]\n",
    "        self.data_subset_ir=[]\n",
    "        self.data_subset_value_counts=[]\n",
    "        if dsc:\n",
    "            self.dsc()\n",
    "        else:\n",
    "            self.nodsc()\n",
    "        'DSE:Data Subset Extension'\n",
    "        self.data_subset_extension=[]\n",
    "        if dse:\n",
    "            self.dse()\n",
    "        else:\n",
    "            self.nodse()\n",
    "        'DFE:Data Feature Emphasis'\n",
    "        self.data_subset_extension_emphasis=[]\n",
    "        if dfe:\n",
    "            self.dfe()\n",
    "        # else:\n",
    "        #     self.nodfe()\n",
    "        \n",
    "        'CSL:Cost-Sensitive Learning'\n",
    "        self.data_subset_metacost=[]\n",
    "        if csl:\n",
    "            self.csl()\n",
    "        \n",
    "    'DSC:Data Subset Construction'\n",
    "    def dsc(self):\n",
    "        for index in range(self.target_types):\n",
    "            temp_df=pd.concat([self.feature,self.target_ordinal],axis='columns',join='inner')\n",
    "            temp_df[self.label_name]=np.where(temp_df[self.label_name]==index,1,0)\n",
    "\n",
    "            self.data_subset_value_counts.append(temp_df[self.label_name].value_counts())\n",
    "\n",
    "            self.data_subset.append(temp_df)\n",
    "            self.data_subset_ir.append(self.data_subset_value_counts[index][0]/self.data_subset_value_counts[index][1])\n",
    "    \n",
    "    'NODSC:No Data Subset Construction'\n",
    "    def nodsc(self):\n",
    "        temp_df=pd.concat([self.feature,self.target_ordinal],axis='columns',join='inner')\n",
    "        temp_value_counts=temp_df[self.label_name].value_counts()\n",
    "        self.data_subset.append(temp_df)\n",
    "        self.data_subset_ir.append(temp_value_counts[0]/temp_value_counts[1])\n",
    "\n",
    "    'DSE:Data Subset Extension'\n",
    "    def dse(self):\n",
    "        for index in range(len(self.data_subset)):\n",
    "            temp_data_subset_extension=[]\n",
    "            if self.data_subset_ir[index]>=self.ir_threshold:\n",
    "                data_subset_extension_minor_size=self.data_subset_value_counts[1]\n",
    "                data_subset_extension_major_minor_ratio=round(self.data_subset_value_counts[index][0]/self.data_subset_value_counts[index][1])\n",
    "                print(data_subset_extension_major_minor_ratio)\n",
    "                \n",
    "                minor_data=self.data_subset[index][self.data_subset[index][self.label_name]==1]\n",
    "                major_data=self.data_subset[index][self.data_subset[index][self.label_name]==0]\n",
    "                \n",
    "                major_data_shuffled=shuffle(major_data)\n",
    "                \n",
    "                '进行划分,若数据不够进行一次划分则全部选用'\n",
    "                major_chunks=np.array_split(major_data_shuffled, data_subset_extension_major_minor_ratio)\n",
    "                for major_chunk in major_chunks:\n",
    "                    temp_data_subset_extension_base=pd.concat([minor_data, major_chunk])\n",
    "                    temp_data_subset_extension.append(temp_data_subset_extension_base)\n",
    "            else:\n",
    "                temp_data_subset_extension.append(self.data_subset[index])\n",
    "                \n",
    "            self.data_subset_extension.append(temp_data_subset_extension)\n",
    "    \n",
    "    'NODSE:No Data Subset Extension'\n",
    "    def nodse(self):\n",
    "        for index in range(len(self.data_subset)):\n",
    "            temp_data_subset_extension=[]\n",
    "            temp_data_subset_extension.append(self.data_subset[index])\n",
    "            self.data_subset_extension.append(temp_data_subset_extension)\n",
    "            \n",
    "    'DFE:Data Feature Emphasis'\n",
    "    def dfe(self):\n",
    "        for index in range(len(self.data_subset_extension)):\n",
    "            temp_subset_lmnn=LMNN(n_neighbors=5,learn_rate=1e-6,random_state=42)\n",
    "            temp_subset_lmnn.fit(self.data_subset[index].loc[:,self.feature_name],self.data_subset[index].loc[:,self.label_name])\n",
    "            \n",
    "            temp_data_subset_extension_emphasis=[]\n",
    "            \n",
    "            for index_1 in range(len(self.data_subset_extension[index])):\n",
    "                temp_data=self.data_subset_extension[index][index_1]\n",
    "                temp_x=temp_data.loc[:,self.feature_name]\n",
    "                temp_y=temp_data.loc[:,self.label_name]\n",
    "\n",
    "                temp_x_lmnn=temp_subset_lmnn.transform(temp_x)\n",
    "        # self.target_ordinal=pd.DataFrame(self.target_o_encoder.fit_transform(self.target.to_frame()),columns=[self.label_name])                \n",
    "                temp_data_subset_extension_emphasis.append(pd.concat([pd.DataFrame(temp_x_lmnn,columns=[self.feature_name]),temp_y],axis='columns',join='inner'))\n",
    "            \n",
    "            self.data_subset_extension_emphasis.append(temp_data_subset_extension_emphasis)\n",
    "                                 \n",
    "    def nodfe(self):\n",
    "        self.data_subset_extension_emphasis=self.data_subset_extension\n",
    "                                 \n",
    "    'CSL:Cost-Sensitive Learning'\n",
    "    def csl(self):\n",
    "        for index in range(len(self.data_subset_extension)):\n",
    "            temp_subset_metacost=[]\n",
    "            for index_1 in range(len(self.data_subset_extension[index])):\n",
    "                temp_knn=KNeighborsClassifier(n_neighbors=5)\n",
    "                a=1\n",
    "                b=1\n",
    "                temp_cost=np.array([[0, a],[b, 0]])\n",
    "                temp_csl=MetaCost(self.data_subset_extension_emphasis[index][index_1],temp_knn,temp_cost).fit(self.label_name,len(self.label_name))\n",
    "                temp_subset_metacost.append(temp_csl)\n",
    "                \n",
    "            self.subset_matacost.append(temp_subset_metacost)\n",
    "            \n",
    "    'EL:Ensemble Learning'\n",
    "    # def el(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "53051b28-02af-450e-8b72-bf2c64aa2877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imsplit(data_original,label_name,target_threshold=0.03):\n",
    "    '对原始数据的标签进行统计'\n",
    "    target_counts=data_original[label_name].value_counts(normalize=True)\n",
    "\n",
    "    'target_selected是index属性的,用此来筛选数据'\n",
    "    target_selected=target_counts[target_counts>=target_threshold].index\n",
    "\n",
    "    'data_selected用于存储筛选后的数据'\n",
    "    data_selected=data_original[data_original[label_name].isin(target_selected)]\n",
    "\n",
    "    return data_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c47a9404-b03f-4c7a-b979-98ed233d66e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Barton_Zhang\\AppData\\Local\\Temp\\ipykernel_9672\\1418394320.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_ori=pd.read_sql(sql,mydb)\n"
     ]
    }
   ],
   "source": [
    "sql='select * from trainingdata'\n",
    "data_ori=pd.read_sql(sql,mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6848081e-1216-4ed6-bff5-caa263449a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(data_ori)\n",
    "data_ori=imsplit(data_ori,label_name='y1',target_threshold=0.03)\n",
    "data_train,data_test=train_test_split(data_ori,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13cfeccf-8a98-4e27-9a98-18abae27f066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "11\n",
      "8\n",
      "7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m imc\u001b[38;5;241m=\u001b[39m\u001b[43mimclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx11\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx12\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdsc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 78\u001b[0m, in \u001b[0;36mimclassifier.__init__\u001b[1;34m(self, data_original, feature_name, label_name, ir_threshold, dsc, dse, dfe, csl, el)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_subset_metacost\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m csl:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 160\u001b[0m, in \u001b[0;36mimclassifier.csl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     b\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    159\u001b[0m     temp_cost\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, a],[b, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m--> 160\u001b[0m     temp_csl\u001b[38;5;241m=\u001b[39mMetaCost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_subset_extension_emphasis[index][index_1],temp_knn,temp_cost)\u001b[38;5;241m.\u001b[39mfit(\u001b[43mlabel_name\u001b[49m,\u001b[38;5;28mlen\u001b[39m(label_name))\n\u001b[0;32m    161\u001b[0m     temp_subset_metacost\u001b[38;5;241m.\u001b[39mappend(temp_csl)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset_matacost\u001b[38;5;241m.\u001b[39mappend(temp_subset_metacost)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_name' is not defined"
     ]
    }
   ],
   "source": [
    "imc=imclassifier(data_ori,feature_name=['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12'],label_name='y1',dsc=True,dse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "067b242b-f3c2-49d2-96e1-344d4b8458aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'imclassifier' object has no attribute 'subset_matacost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mimc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset_matacost\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'imclassifier' object has no attribute 'subset_matacost'"
     ]
    }
   ],
   "source": [
    "imc.subset_matacost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3593f4e1-b9b4-468d-aeb4-84cfc5c09743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 0 = 23\n",
      "no: 1 = 1\n",
      "no: 2 = 1\n",
      "no: 3 = 11\n",
      "no: 4 = 8\n",
      "no: 5 = 7\n"
     ]
    }
   ],
   "source": [
    "# len(imc.data_subset_extension)\n",
    "sum=0\n",
    "for index in range(len(imc.data_subset_extension)):\n",
    "    print(\"no:\",index,\"=\",len(imc.data_subset_extension[index]))\n",
    "    sum+=len(imc.data_subset_extension[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad57a581-50fc-4170-91b9-da5b46d4cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c227d8-aa2c-46cd-90ed-9fabdfdae464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三资企业 \t 23.46875 \t 0.03841536614645858\n",
      "不就业拟升学 \t 77.3 \t 0.012004801920768308\n",
      "其他企业 \t 1.7864768683274022 \t 0.34813925570228094\n",
      "其他暂不就业 \t 70.18181818181819 \t 0.014405762304921969\n",
      "升学 \t 3.078125 \t 0.23649459783913565\n",
      "国有企业 \t 11.836065573770492 \t 0.07683073229291716\n",
      "待就业 \t 8.433734939759036 \t 0.10084033613445378\n",
      "机关 \t 86.0 \t 0.010804321728691477\n",
      "科研助理 \t 77.3 \t 0.012004801920768308\n",
      "自由职业 \t 7.329787234042553 \t 0.12004801920768307\n"
     ]
    }
   ],
   "source": [
    "# imc.data_subset[0]['y1'].value_counts()\n",
    "\n",
    "for i in range(len(imc.data_subset_ir)):\n",
    "    print(imc.target_name[0][i],\"\\t\",imc.data_subset_ir[i],\"\\t\",imc.target_counts[imc.target_name[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72930c27-991d-4417-a514-7e2c027ea1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 0\n",
      "index= 1\n",
      "index= 2\n",
      "index= 3\n",
      "index= 4\n",
      "index= 5\n",
      "index= 6\n",
      "index= 7\n",
      "index= 8\n",
      "index= 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(imc.target_types):\n",
    "    print(\"index=\",i)\n",
    "    # print(\"index=\",i,imc.data_subset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b09033ea-0cb1-4fac-b20c-757e2d0fae15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83bf08-8289-4b9f-a408-5a9fec22f7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonEnv3.10",
   "language": "python",
   "name": "pythonenv3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
